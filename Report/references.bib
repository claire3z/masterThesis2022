% Encoding: ISO8859_1

% Intro

@article{MLinUKFin2019,
Author = {Jung, Carsten and Mueller, Henrike and Pedemonte, Simone and Plances, Simone and Thew, Oliver},
Journal = {Bank of England (BOE) and Financial Conduct Authority (FCA)},
Year = {2019},
Month = {October},
Title = {Machine learning in UK financial services},
}



@Article{TextMiningFinancialSector2019,
AUTHOR = {Pejic Bach, Mirjana and Krstic, Zivko and Seljan, Sanja and Turulja, Lejla},
TITLE = {Text Mining for Big Data Analysis in Financial Sector: A Literature Review},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {1277},
URL = {https://www.mdpi.com/2071-1050/11/5/1277},
ISSN = {2071-1050},
ABSTRACT = {Big data technologies have a strong impact on different industries, starting from the last decade, which continues nowadays, with the tendency to become omnipresent. The financial sector, as most of the other sectors, concentrated their operating activities mostly on structured data investigation. However, with the support of big data technologies, information stored in diverse sources of semi-structured and unstructured data could be harvested. Recent research and practice indicate that such information can be interesting for the decision-making process. Questions about how and to what extent research on data mining in the financial sector has developed and which tools are used for these purposes remains largely unexplored. This study aims to answer three research questions: (i) What is the intellectual core of the field? (ii) Which techniques are used in the financial sector for textual mining, especially in the era of the Internet, big data, and social media? (iii) Which data sources are the most often used for text mining in the financial sector, and for which purposes? In order to answer these questions, a qualitative analysis of literature is carried out using a systematic literature review, citation and co-citation analysis.},
DOI = {10.3390/su11051277}
}



@article{FadorFuture2018,
author = {Lewis, Craig and Young, Steven},
year = {2018},
month = {12},
title = {Fad or Future? Automated Analysis of Financial Text and Its implications for corporate Reporting},
Journal = {Forthcoming in Accounting and Business Research},
abstract = {This paper describes the current state of natural language processing (NLP) as it applies to corporate reporting. We document dramatic increases in the quantity of verbal content that are an integral part of company reporting packages as well as the evolution of text analytic approaches that are being employed to analyse this content. We provide intuitive descriptions of the leading analytic approaches that are applied in the academic accounting and finance literatures. This discussion includes key word searches and counts, attribute dictionaries, naive Bayesian classification, cosine similarity, and latent Dirichlet allocation. We also discuss how increasing interest in NLP processing of the corporate reporting package could and should influence financial reporting regulation and note that textual analysis is currently is more of an afterthought, if it is even considered. Opportunities for improving the usefulness of NLP processing are discussed as well as possible impediments.}
}



@inproceedings{MLinFinance2021,
  title={Machine Learning Methods in Finance: Recent Applications and Prospects},
  author={Daniel Hoang and Kevin Wiegratz},
  year={2021},
abstract = {We study how researchers can apply machine learning (ML) methods in finance. We first establish that the three distinct categories of ML (supervised learning, unsupervised learning, reinforcement learning and others) address fundamentally different problems than traditional econometric approaches. Then, we review the current state of research of ML in finance and identify three archetypes of applications: i) the construction of superior and novel measures, ii) the reduction of prediction error, and iii) the extension of the standard econometric toolset. With this taxonomy, we provide an outlook on potential future directions for both researchers and practitioners. We finally apply ML to typical problems in finance. Our results suggest large benefits of ML methods compared to traditional approaches and indicate that ML holds great potential for future research in finance. This draft is preliminary. Comments welcome. Please do not circulate without permission of the authors}
}



% Causality Extraction

@Article{Ali21Survey,
	AUTHOR = {Ali, Wajid and Zuo, Wanli and Ali, Rahman and Zuo, Xianglin and Rahman, Gohar},
	TITLE = {Causality Mining in Natural Languages Using Machine and Deep Learning Techniques: A Survey},
	JOURNAL = {Applied Sciences},
	VOLUME = {11},
	YEAR = {2021},
	NUMBER = {21},
	ARTICLE-NUMBER = {10064},
	URL = {https://www.mdpi.com/2076-3417/11/21/10064},
	ISSN = {2076-3417},
	ABSTRACT = {The era of big textual corpora and machine learning technologies have paved the way for researchers in numerous data mining fields. Among them, causality mining (CM) from textual data has become a significant area of concern and has more attention from researchers. Causality (cause-effect relations) serves as an essential category of relationships, which plays a significant role in question answering, future events predication, discourse comprehension, decision making, future scenario generation, medical text mining, behavior prediction, and textual prediction entailment. While, decades of development techniques for CM are still prone to performance enhancement, especially for ambiguous and implicitly expressed causalities. The ineffectiveness of the early attempts is mainly due to small, ambiguous, heterogeneous, and domain-specific datasets constructed by manually linguistic and syntactic rules. Many researchers have deployed shallow machine learning (ML) and deep learning (DL) techniques to deal with such datasets, and they achieved satisfactory performance. In this survey, an effort has been made to address a comprehensive review of some state-of-the-art shallow ML and DL approaches in CM. We present a detailed taxonomy of CM and discuss popular ML and DL approaches with their comparative weaknesses and strengths, applications, popular datasets, and frameworks. Lastly, the future research challenges are discussed with illustrations of how to transform them into productive future research directions.},
	DOI = {10.3390/app112110064}
}

@inproceedings{Xu20Review,
    title = "A Review of Dataset and Labeling Methods for Causality Extraction",
    author = "Xu, Jinghang  and
      Zuo, Wanli  and
      Liang, Shining  and
      Zuo, Xianglin",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.133",
    doi = "10.18653/v1/2020.coling-main.133",
    pages = "1519--1531",
    abstract = "Causality represents the most important kind of correlation between events. Extracting causali-ty from text has become a promising hot topic in NLP. However, there is no mature research systems and datasets for public evaluation. Moreover, there is a lack of unified causal sequence label methods, which constitute the key factors that hinder the progress of causality extraction research. We survey the limitations and shortcomings of existing causality research field com-prehensively from the aspects of basic concepts, extraction methods, experimental data, and la-bel methods, so as to provide reference for future research on causality extraction. We summa-rize the existing causality datasets, explore their practicability and extensibility from multiple perspectives and create a new causal dataset ESC. Aiming at the problem of causal sequence labeling, we analyse the existing methods with a summarization of its regulation and propose a new causal label method of core word. Multiple candidate causal label sequences are put for-ward according to label controversy to explore the optimal label method through experiments, and suggestions are provided for selecting label method.",
}

@misc{Yang21Survey,
  doi = {10.48550/ARXIV.2101.06426},
  url = {https://arxiv.org/abs/2101.06426},
  author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
  title = {A Survey on Extraction of Causal Relations from Natural Language Text},
  publisher = {arXiv},  
  year = {2021},
}


@misc{Asghar16Survey,
  doi = {10.48550/ARXIV.1605.07895},  
  url = {https://arxiv.org/abs/1605.07895},
  author = {Asghar, Nabiha},
  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey},
  publisher = {arXiv},  
  year = {2016},
}





@phdthesis{DunietzThesis18,
  title={Annotating and Automatically Tagging Constructions of Causal Language},
  author={Jesse Dunietz},
  year={2018},
  school={Computer Science Department, Carnegie Mellon University},
}


@phdthesis{KhooThesis95,
  title={Automatic identification of causal relations in text and their use
for improving precision in information retrieval },
  author={Khoo, Christopher S. G.},
  year={1995},
  school={Computational Linguistics, University of Arizona}
}

@article{Khoo98,
    author = {Khoo, Christopher S. G. and Kornfilt, Jaklin and Oddy, Robert N. and Myaeng, Sung Hyon},
    title = "{Automatic Extraction of Cause-Effect Information from Newspaper Text Without Knowledge-based Inferencing}",
    journal = {Literary and Linguistic Computing},
    volume = {13},
    number = {4},
    pages = {177-186},
    year = {1998},
    month = {12},
    abstract = "{This study investigated how effectively cause-effect information can be extracted from newspaper text using a simple computational method (i.e. without knowledge-based inferencing and without full parsing of sentences). An automatic method was developed for identifying and extracting cause-effect information in Wall Street Journal text using linguistic clues and pattern matching. The set of linguistic patterns used for identifying causal relationships was based on a through review of the literature and on an analysis of sample sentences from the Wall Street Journal. The cause-effect information extracted using the method was compared with that identified by two human judges. The program successfully extracted ˜68\% of the causal relationships identified by both judges (the intersection of the two sets of causal relationships identified by the judges.) Of the instances that the computer program identified as causal relationships, ˜25\% were identified by both judges, and 64\% were identified by at least one of the judges. Problems encountered are discussed.}",
    issn = {0268-1145},
    doi = {10.1093/llc/13.4.177},
    url = {https://doi.org/10.1093/llc/13.4.177},
    eprint = {https://academic.oup.com/dsh/article-pdf/13/4/177/10888761/177.pdf},
}


@article{Khoo01,
  author    = {Christopher S. G. Khoo and
               Sung{-}Hyon Myaeng and
               Robert N. Oddy},
  title     = {Using cause-effect relations in text to improve information retrieval
               precision},
  journal   = {Inf. Process. Manag.},
  volume    = {37},
  number    = {1},
  pages     = {119--145},
  year      = {2001},
  url       = {https://doi.org/10.1016/S0306-4573(00)00022-4},
  doi       = {10.1016/S0306-4573(00)00022-4},
  timestamp = {Fri, 21 Feb 2020 13:10:35 +0100},
  biburl    = {https://dblp.org/rec/journals/ipm/KhooMO01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Girju02,
  title={Mining Answers for Causation Questions},
  author={Roxana Girju and Dan I. Moldovan},
  year={2002}
}

@inproceedings{Girju03,
author = {Girju, Roxana},
title = {Automatic Detection of Causal Relations for Question Answering},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119312.1119322},
doi = {10.3115/1119312.1119322},
abstract = {Causation relations are a pervasive feature of human language. Despite this, the automatic acquisition of causal information in text has proved to be a difficult task in NLP. This paper provides a method for the automatic detection and extraction of causal relations. We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering. We devised a classification of causal questions and tested the procedure on a QA system.},
booktitle = {Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering - Volume 12},
pages = {76-83},
numpages = {8},
location = {Sapporo, Japan},
series = {MultiSumQA '03}
}


@article{SEKE05,
  title={Extracting causation knowledge from natural language texts},
  author={Ki Chan and Wai Lam},
  journal={International Journal of Intelligent Systems},
  year={2005},
  volume={20},
abstract = {SEKE is a semantic expectation‐based knowledge extraction system for extracting causation knowledge from natural language texts. It is inspired by human behavior on analyzing texts and capturing information with semantic expectations. The framework of SEKE consists of different kinds of generic templates organized in a hierarchical fashion. There are semantic templates, sentence templates, reason templates, and consequence templates. The design of templates is based on the expected semantics o}
}


@InProceedings{IttooBouma11,
author="Ittoo, Ashwin
and Bouma, Gosse",
editor="Mu{\~{n}}oz, Rafael
and Montoyo, Andr{\'e}s
and M{\'e}tais, Elisabeth",
title="Extracting Explicit and Implicit Causal Relations from Sparse, Domain-Specific Texts",
booktitle="Natural Language Processing and Information Systems",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="52-63",
abstract="Various supervised algorithms for mining causal relations from large corpora exist. These algorithms have focused on relations explicitly expressed with causal verbs, e.g. to cause. However, the challenges of extracting causal relations from domain-specific texts have been overlooked. Domain-specific texts are rife with causal relations that are implicitly expressed using verbal and non-verbal patterns, e.g. reduce, drop in, due to. Also, readily-available resources to support supervised algorithms are inexistent in most domains. To address these challenges, we present a novel approach for causal relation extraction. Our approach is minimally-supervised, alleviating the need for annotated data. Also, it identifies both explicit and implicit causal relations. Evaluation results revealed that our technique achieves state-of-the-art performance in extracting causal relations from domain-specific, sparse texts. The results also indicate that many of the domain-specific relations were unclassifiable in existing taxonomies of causality.",
isbn="978-3-642-22327-3"
}


@inproceedings{Sorgente13,
  title={Automatic Extraction of Cause-Effect Relations in Natural Language Text},
  author={Antonio Sorgente and Giuseppe Vettigli and Francesco Mele},
  booktitle={DART@AI*IA},
  year={2013}
}


@inproceedings{Rink10,
    title = "{UTD}: Classifying Semantic Relations by Combining Lexical and Semantic Resources",
    author = "Rink, Bryan  and
      Harabagiu, Sanda",
    booktitle = "Proceedings of the 5th International Workshop on Semantic Evaluation",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S10-1057",
    pages = "256--259",
}


@article{Zhao16,
title = {Event causality extraction based on connectives analysis},
journal = {Neurocomputing},
volume = {173},
pages = {1943-1950},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.09.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215013995},
author = {Sendong Zhao and Ting Liu and Sicheng Zhao and Yiheng Chen and Jian-Yun Nie},
keywords = {Causality extraction, Connective categorization, Hidden Naive Bayes, Text mining},
abstract = {Causality is an important type of relation which is crucial in numerous tasks, such as predicting future events, generating scenario, question answering, textual entailment and discourse comprehension. Therefore, causality extraction is a fundamental task in text mining. Many efforts have been dedicated to extracting causality from texts utilizing patterns, constraints and machine learning techniques. This paper presents a new Restricted Hidden Naive Bayes model to extract causality from texts. Besides some commonly used features, such as contextual features, syntactic features, position features, we also utilize a new category feature of causal connectives. This new feature is obtained from the tree kernel similarity of sentences containing connectives. In previous studies, the features have been usually assumed to be independent, which is not the case in reality. The advantage of our model lies in its ability to cope with partial interactions among features so as to avoid over-fitting problem on Hidden Naive Bayes model, especially the interaction between the connective category and the syntactic structure of sentences. Evaluation on a public dataset shows that our method goes beyond all the baselines.}
}



@inproceedings{SemEval07Task4,
    title = "{S}em{E}val-2007 Task 04: Classification of Semantic Relations between Nominals",
    author = "Girju, Roxana  and
      Nakov, Preslav  and
      Nastase, Vivi  and
      Szpakowicz, Stan  and
      Turney, Peter  and
      Yuret, Deniz",
    booktitle = "Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007)",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S07-1003",
    pages = "13--18",
}


@inproceedings{SemEval10Task8,
    title = "{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals",
    author = "Hendrickx, Iris  and
      Kim, Su Nam  and
      Kozareva, Zornitsa  and
      Nakov, Preslav  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Pad{\'o}, Sebastian  and
      Pennacchiotti, Marco  and
      Romano, Lorenza  and
      Szpakowicz, Stan",
    booktitle = "Proceedings of the 5th International Workshop on Semantic Evaluation",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S10-1006",
    pages = "33--38",
}


@misc{FinCausal20,
  doi = {10.48550/ARXIV.2012.02505},  
  url = {https://arxiv.org/abs/2012.02505},
  author = {Mariko, Dominique and Akl, Hanna Abi and Labidurie, Estelle and Durfort, Stéphane and de Mazancourt, Hugues and El-Haj, Mahmoud},
  title = {Financial Document Causality Detection Shared Task (FinCausal 2020)},
  publisher = {arXiv},
  year = {2020}
}

@inproceedings{NTUNLPL20,
    title = "{NTUNLPL} at {F}in{C}ausal 2020, Task 2:Improving Causality Detection Using {V}iterbi Decoder",
    author = "Kao, Pei-Wei  and
      Chen, Chung-Chi  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    booktitle = "Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "COLING",
    url = "https://aclanthology.org/2020.fnp-1.11",
    pages = "69--73",
    abstract = "In order to provide an explanation of machine learning models, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage.",
}


@inproceedings{GBE20,
    title = "{GB}e at {F}in{C}ausal 2020, Task 2: Span-based Causality Extraction for Financial Documents",
    author = "Becquin, Guillaume",
    booktitle = "Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "COLING",
    url = "https://aclanthology.org/2020.fnp-1.5",
    pages = "40--44",
    abstract = "This document describes a system for causality extraction from financial documents submitted as part of the FinCausal 2020 Workshop. The main contribution of this paper is a description of the robust post-processing used to detect the number of cause and effect clauses in a document and extract them. The proposed system achieved a weighted-average F1 score of more than 95{\%} for the official blind test set during the post-evaluation phase and exact clauses match for 83{\%} of the documents.",
}


@inproceedings{Kruengkrai17,
  title={Improving Event Causality Recognition with Multiple Background Knowledge Sources Using Multi-Column Convolutional Neural Networks},
  author={Canasai Kruengkrai and Kentaro Torisawa and Chikara Hashimoto and Julien Kloetzer and Jong-Hoon Oh and Masahiro Tanaka},
  booktitle={AAAI},
  year={2017}
}


@article{LiMao19,
title = {Knowledge-oriented convolutional neural network for causal relation extraction from natural language texts},
journal = {Expert Systems with Applications},
volume = {115},
pages = {512-523},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418305177},
author = {Pengfei Li and Kezhi Mao},
keywords = {Natural language processing, Convolutional neural network, Relation extraction, Causal relationship, Lexical knowledge base},
abstract = {Causal relation extraction is a challenging yet very important task for Natural Language Processing (NLP). There are many existing approaches developed to tackle this task, either rule-based (non-statistical) or machine-learning-based (statistical) method. For rule-based method, extensive manual work is required to construct handcrafted patterns, however, the precision and recall are low due to the complexity of causal relation expressions in natural language. For machine-learning-based method, current approaches either rely on sophisticated feature engineering which is error-prone, or rely on large amount of labeled data which is impractical for causal relation extraction problem. To address the above issues, we propose a Knowledge-oriented Convolutional Neural Network (K-CNN) for causal relation extraction in this paper. K-CNN consists of a knowledge-oriented channel that incorporates human prior knowledge to capture the linguistic clues of causal relationship, and a data-oriented channel that learns other important features of causal relation from the data. The convolutional filters in knowledge-oriented channel are automatically generated from lexical knowledge bases such as WordNet and FrameNet. We propose filter selection and clustering techniques to reduce dimensionality and improve the performance of K-CNN. Furthermore, additional semantic features that are useful for identifying causal relations are created. Three datasets have been used to evaluate the ability of K-CNN to effectively extract causal relation from texts, and the model outperforms current state-of-art models for relation extraction.}
}



@inproceedings{dasgupta18,
    title = "Automatic Extraction of Causal Relations from Text using Linguistically Informed Deep Neural Networks",
    author = "Dasgupta, Tirthankar  and
      Saha, Rupsa  and
      Dey, Lipika  and
      Naskar, Abir",
    booktitle = "Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5035",
    doi = "10.18653/v1/W18-5035",
    pages = "306--316",
    abstract = "In this paper we have proposed a linguistically informed recursive neural network architecture for automatic extraction of cause-effect relations from text. These relations can be expressed in arbitrarily complex ways. The architecture uses word level embeddings and other linguistic features to detect causal events and their effects mentioned within a sentence. The extracted events and their relations are used to build a causal-graph after clustering and appropriate generalization, which is then used for predictive purposes. We have evaluated the performance of the proposed extraction model with respect to two baseline systems,one a rule-based classifier, and the other a conditional random field (CRF) based supervised model. We have also compared our results with related work reported in the past by other authors on SEMEVAL data set, and found that the proposed bi-directional LSTM model enhanced with an additional linguistic layer performs better. We have also worked extensively on creating new annotated datasets from publicly available data, which we are willing to share with the community.",
}


@InProceedings{Chen20,
author="Chen, Dian
and Cao, Yixuan
and Luo, Ping",
editor="Zhu, Xiaodan
and Zhang, Min
and Hong, Yu
and He, Ruifang",
title="Pairwise Causality Structure: Towards Nested Causality Mining on Financial Statements",
booktitle="Natural Language Processing and Chinese Computing",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="725--737",
abstract="Causality mining, which aims to find cause-effect relations in text, is an important yet challenging problem in natural language understanding. The extraction of causal relations is beneficial to practitioners in document-intensive industries. For instance, it enables investors and regulators in financial industries to quickly understand the correlation between events in financial statements. However, this problem is difficult since the expression of causality is diverse, and more importantly, nested. Specifically, causality often has a nested structure, where a pair of cause-effect can be the cause of another higher-level causality. Recent works deal with this problem by a bottom-up relation extraction solution, but it performs worse for relations on higher levels. In this study, we find that the nested causality structure can be transformed into a graph of pairwise causality between sentence segments. Then we propose a two-step solution: first, a segmenter disassembles a sentence into segments by detecting causality connectives; second, a relation classifier predicts whether a pair of segments has cause-effect relation or not. Two modules above are trained jointly in our proposed Causality Detection Network (CDNet). On a large dataset we collect, the precision of our model reaches 92.11{\%} and the recall reaches 93.07{\%} for this task. Compared with the existing state-of-the-art solution, the precision of our model is improved by 3.28{\%} and 3.03{\%} for recall. We also observe that the percentage of exactly correct sentences from prediction is 74.26{\%} without post-processing, indicating the hardness of our problem and space for improvement.",
isbn="978-3-030-60450-9"
}


@inproceedings{causenet2020,
	author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngonga Ngomo, Axel-Cyrille and Potthast, Martin},
	title = {CauseNet: Towards a Causality Graph Extracted from the Web},
	year = {2020},
	isbn = {9781450368599},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3340531.3412763},
	doi = {10.1145/3340531.3412763},
	abstract = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.},
	booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
	pages = {3023 - 3030},
	numpages = {8},
	keywords = {information extraction, knowledge graph, causality},
	location = {Virtual Event, Ireland},
	series = {CIKM '20}
}


@article{Li21BiLSTMCRF,
	doi = {10.1016/j.neucom.2020.08.078},
	url = {https://doi.org/10.1016\%2Fj.neucom.2020.08.078},
	year = {2021},
	month = {jan},
	publisher = {Elsevier {BV}},
	volume = {423},
	pages = {207--219},
	author = {Zhaoning Li and Qi Li and Xiaotian Zou and Jiangtao Ren},
	title = {Causality extraction based on self-attentive {BiLSTM}-{CRF} with transferred embeddings},  
	journal = {Neurocomputing}
}


% Word Embedding
@misc{BERT2018,
  doi = {10.48550/ARXIV.1810.04805},  
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Ferrone20Survey,
	doi = {10.3389/frobt.2019.00153},  
	url = {https://doi.org/10.3389\%2Ffrobt.2019.00153},
	year = 2020,
	month = {jan},
	publisher = {Frontiers Media {SA}},
	volume = {6},
	author = {Lorenzo Ferrone and Fabio Massimo Zanzotto},
	title = {Symbolic, Distributed, and Distributional Representations for Natural Language Processing in the Era of Deep Learning: A Survey},
	journal = {Frontiers in Robotics and {AI}},
abstract={Natural language is inherently a discrete symbolic representation of human knowledge. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: discrete symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and discrete symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols may certainly lead to radically new deep learning networks. In this paper we make a survey that aims to renew the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how discrete symbols are represented inside neural networks.}
}


@article{Cartuyvels21Survey,
	doi = {10.1016/j.aiopen.2021.07.002},  
	url = {https://doi.org/10.1016\%2Fj.aiopen.2021.07.002},
	year = 2021,
	publisher = {Elsevier {BV}},
	volume = {2},
	pages = {143--159},
	author = {Ruben Cartuyvels and Graham Spinks and Marie-Francine Moens},
	title = {Discrete and continuous representations and processing in deep learning: Looking forward},
	journal = {{AI} Open},
	abstract={Discrete and continuous representations of content (e.g., of language or images) have interesting properties to be explored for the understanding of or reasoning with this content by machines. This position paper puts forward our opinion on the role of discrete and continuous representations and their processing in the deep learning field. Current neural network models compute continuous-valued data. Information is compressed into dense, distributed embeddings. By stark contrast, humans use discrete symbols in their communication with language. Such symbols represent a compressed version of the world that derives its meaning from shared contextual information. Additionally, human reasoning involves symbol manipulation at a cognitive level, which facilitates abstract reasoning, the composition of knowledge and understanding, generalization and efficient learning. Motivated by these insights, in this paper we argue that combining discrete and continuous representations and their processing will be essential to build systems that exhibit a general form of intelligence. We suggest and discuss several avenues that could improve current neural networks with the inclusion of discrete elements to combine the advantages of both types of representations.}

}


@misc{WordEmbSurvey19,
  doi = {10.48550/ARXIV.1901.09069},  
  url = {https://arxiv.org/abs/1901.09069},
  author = {Almeida, Felipe and Xexéo, Geraldo},
  title = {Word Embeddings: A Survey},
  publisher = {arXiv},
  year = {2019},
  abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks}
}


@article{Salton75,
author = {Salton, G. and Wong, A. and Yang, C. S.},
title = {A Vector Space Model for Automatic Indexing},
year = {1975},
issue_date = {Nov. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/361219.361220},
doi = {10.1145/361219.361220},
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
journal = {Commun. ACM},
month = {nov},
pages = {613-620},
numpages = {8},
keywords = {content analysis, automatic information retrieval, document space, automatic indexing}
}


@article{Joos50,
  title={Description of Language Design},
  author={Martin Joos},
  journal={Journal of the Acoustical Society of America},
  year={1950},
  volume={22},
  pages={701-707}
}

@article{harris54,
  added-at = {2020-05-20T16:56:27.000+0200},
  author = {Harris, Zellig},
  biburl = {https://www.bibsonomy.org/bibtex/252e7950c31610617170d71c320f2252e/ghagerer},
  doi = {10.1007/978-94-009-8467-7_1},
  interhash = {a23596808b6273076e1259dedca16330},
  intrahash = {52e7950c31610617170d71c320f2252e},
  journal = {Word},
  keywords = {bag-of-words},
  number = {2-3},
  pages = {146--162},
  publisher = {Taylor \& Francis},
  timestamp = {2020-06-24T14:53:20.000+0200},
  title = {Distributional structure},
  url = {https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1},
  volume = 10,
  year = 1954
}


@article{firth57,
  abstract = {Reprinted in:  Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London. },
  added-at = {2008-05-14T00:52:58.000+0200},
  address = {Oxford},
  author = {Firth, J. R.},
  biburl = {https://www.bibsonomy.org/bibtex/25e3d6c72cdd123a638f71886d78f3c1e/brightbyte},
  booktitle = {Studies in Linguistic Analysis (special volume of the Philological Society)},
  interhash = {b4f769667fdd195b4a75f61f6388a52e},
  intrahash = {5e3d6c72cdd123a638f71886d78f3c1e},
  keywords = {classic linguistics meanign relatedness semantic},
  pages = {1-32},
  publisher = {The Philological Society},
  timestamp = {2009-01-23T09:58:50.000+0100},
  title = {A synopsis of linguistic theory 1930-55.},
  volume = {1952-59},
  year = 1957
}


@book{Jurafsky2009,
  abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
  added-at = {2013-04-24T13:46:19.000+0200},
  address = {Upper Saddle River, N.J.},
  author = {Jurafsky, Dan and Martin, James H.},
  biburl = {https://www.bibsonomy.org/bibtex/2fb7fa20679ebb9d69d27d7c9682fd774/lopusz_kdd},
  description = {Speech and Language Processing (2nd Edition): Daniel Jurafsky, James H. Martin: 9780131873216: Amazon.com: Books},
  interhash = {5f4a309a36c3da5e3becbf0ac5d88413},
  intrahash = {fb7fa20679ebb9d69d27d7c9682fd774},
  isbn = {9780131873216 0131873210},
  keywords = {language},
  publisher = {Pearson Prentice Hall},
  refid = {213375806},
  timestamp = {2013-04-24T13:46:19.000+0200},
  title = {Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition},
  url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd_bxgy_b_img_y},
  year = 2009
}

@ARTICLE{Bengio2003,
    author = {Yoshua Bengio and Réjean Ducharme and Pascal Vincent and Christian Jauvin},
    title = {A Neural Probabilistic Language Model},
    journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
    year = {2003},
    volume = {3},
    pages = {1137--1155}
}


@misc{w2v2013,
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Estimation of Word Representations in Vector Space}, 
  publisher = {arXiv},  
  year = {2013},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{glove2014,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}



@misc{elmo2018,
  doi = {10.48550/ARXIV.1802.05365},  
  url = {https://arxiv.org/abs/1802.05365},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep contextualized word representations},
  publisher = {arXiv},
  year = {2018},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}


@inproceedings{flair2019,
    title = "{FLAIR}: An Easy-to-Use Framework for State-of-the-Art {NLP}",
    author = "Akbik, Alan  and
      Bergmann, Tanja  and
      Blythe, Duncan  and
      Rasul, Kashif  and
      Schweter, Stefan  and
      Vollgraf, Roland",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-4010",
    doi = "10.18653/v1/N19-4010",
    pages = "54--59",
    abstract = "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to {``}mix and match{''} various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a {``}model zoo{''} of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",
}


@inproceedings{NPcomposing19,
    title = "Composing Noun Phrase Vector Representations",
    author = "Kalouli, Aikaterini-Lida  and
      de Paiva, Valeria  and
      Crouch, Richard",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4311",
    doi = "10.18653/v1/W19-4311",
    pages = "84--95",
    abstract = "Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of word embeddings and how to learn them, it is still unclear which representations can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex deep architectures. In this work, we propose two novel constraints for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a noun phrase should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions{'} selection in a way that specific semantic features captured by the phrase{'}s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these constraints lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.",
}


@misc{PhraseBERT21,
  doi = {10.48550/ARXIV.2109.06304},
  url = {https://arxiv.org/abs/2109.06304}, 
  author = {Wang, Shufan and Thompson, Laure and Iyyer, Mohit},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International},
abstract={Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.}
}


@article{Mitchell2010,
  title={Composition in Distributional Models of Semantics},
  author={Jeff Mitchell and Mirella Lapata},
  journal={Cognitive science},
  year={2010},
  volume={34 8},
  pages={1388-429},
abstract={Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little}
}

@inproceedings{baroni-zamparelli-2010,
    title = "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space",
    author = "Baroni, Marco  and
      Zamparelli, Roberto",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D10-1115",
    pages = "1183--1193",
}

@inproceedings{zanzotto2010,
    title = "Estimating Linear Models for Compositional Distributional Semantics",
    author = "Zanzotto, Fabio Massimo  and
      Korkontzelos, Ioannis  and
      Fallucchi, Francesca  and
      Manandhar, Suresh",
    booktitle = "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)",
    month = aug,
    year = "2010",
    address = "Beijing, China",
    publisher = "Coling 2010 Organizing Committee",
    url = "https://aclanthology.org/C10-1142",
    pages = "1263--1271",
}


@article{yu-dredze-2015,
    title = "Learning Composition Models for Phrase Embeddings",
    author = "Yu, Mo  and
      Dredze, Mark",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1017",
    doi = "10.1162/tacl_a_00135",
    pages = "227--242",
    abstract = "Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.",
}


@inproceedings{zhu18sentence,
    title = "Exploring Semantic Properties of Sentence Embeddings",
    author = "Zhu, Xunjie  and
      Li, Tingfeng  and
      de Melo, Gerard",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2100",
    doi = "10.18653/v1/P18-2100",
    pages = "632--637",
    abstract = "Neural vector representations are ubiquitous throughout all subfields of NLP. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.",
}


@inproceedings{zhou17,
    title = "Learning Phrase Embeddings from Paraphrases with {GRU}s",
    author = "Zhou, Zhihao  and
      Huang, Lifu  and
      Ji, Heng",
    booktitle = "Proceedings of the First Workshop on Curation and Applications of Parallel and Comparable Corpora",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/W17-5603",
    pages = "16--23",
    abstract = "Learning phrase representations has been widely explored in many Natural Language Processing tasks (e.g., Sentiment Analysis, Machine Translation) and has shown promising improvements. Previous studies either learn non-compositional phrase representations with general word embedding learning techniques or learn compositional phrase representations based on syntactic structures, which either require huge amounts of human annotations or cannot be easily generalized to all phrases. In this work, we propose to take advantage of large-scaled paraphrase database and present a pairwise-GRU framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks.",
}


@inproceedings{White2015,
author = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
title = {How Well Sentence Embeddings Capture Meaning},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838932},
doi = {10.1145/2838931.2838932},
abstract = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications.In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {9},
numpages = {8},
keywords = {word embeddings, Semantic vector space representations, sentence embeddings, semantic consistency evaluation},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}


@article{Wieting2016,
  title={Towards Universal Paraphrastic Sentence Embeddings},
  author={John Wieting and Mohit Bansal and Kevin Gimpel and Karen Livescu},
  journal={CoRR},
  year={2016},
  volume={abs/1511.08198},
abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use.
In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank.}
}



@inproceedings{Arora2017,
  title={A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
  author={Sanjeev Arora and Yingyu Liang and Tengyu Ma},
  booktitle={ICLR},
  year={2017},
abstract={}
}


@misc{sentenceBERT2019,
  doi = {10.48550/ARXIV.1908.10084},
  url = {https://arxiv.org/abs/1908.10084},
  author = {Reimers, Nils and Gurevych, Iryna},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  publisher = {arXiv},
  year = {2019},  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{sentenceEmb2020,
  doi = {10.48550/ARXIV.2011.05864},
  url = {https://arxiv.org/abs/2011.05864},
  author = {Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Sentence Embeddings from Pre-trained Language Models},
  publisher = {arXiv},  
  year = {2020},
  abstract = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. }
}



% Clustering

@article{Jain2010,
title = {Data clustering: 50 years beyond K-means},
journal = {Pattern Recognition Letters},
volume = {31},
number = {8},
pages = {651-666},
year = {2010},
note = {Award winning papers from the 19th International Conference on Pattern Recognition (ICPR)},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167865509002323},
author = {Anil K. Jain},
keywords = {Data clustering, User’s dilemma, Historical developments, Perspectives on clustering, King-Sun Fu prize},
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.}
}


@article{Xu2015Survey,
  title={A Comprehensive Survey of Clustering Algorithms},
  author={Dongkuan Xu and Ying-jie Tian},
  journal={Annals of Data Science},
  year={2015},
  volume={2},
  pages={165-193}
}


@inproceedings{miniKmeans2013,
  title={K-means vs Mini Batch K-means: a comparison},
  author={Javier B{\'e}jar Alonso},
  year={2013}
}


@article{Brown1992,
    title = "Class-Based \textit{n}-gram Models of Natural Language",
    author = "Brown, Peter F.  and
      Della Pietra, Vincent J.  and
      deSouza, Peter V.  and
      Lai, Jenifer C.  and
      Mercer, Robert L.",
    journal = "Computational Linguistics",
    volume = "18",
    number = "4",
    year = "1992",
    url = "https://aclanthology.org/J92-4003",
    pages = "467-480",
}


@inproceedings{Derczynski2015,
  title={Tune Your Brown Clustering, Please},
  author={Leon Derczynski and Sean Chester and Kenneth S. B{\o}gh},
  booktitle={RANLP},
  year={2015}
}


@INPROCEEDINGS{Liang2005,
    author = {Percy Liang},
    title = {Semi-supervised learning for natural language},
    booktitle = {MASTER'S THESIS, MIT},
    year = {2005},
    publisher = {}
}


@inproceedings{ciosici2020,
    title = "Accelerated High-Quality Mutual-Information Based Word Clustering",
    author = "Ciosici, Manuel R.  and
      Assent, Ira  and
      Derczynski, Leon",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.303",
    pages = "2491--2496",
    abstract = "Word clustering groups words that exhibit similar properties. One popular method for this is Brown clustering, which uses short-range distributional information to construct clusters. Specifically, this is a hard hierarchical clustering with a fixed-width beam that employs bi-grams and greedily minimizes global mutual information loss. The result is word clusters that tend to outperform or complement other word representations, especially when constrained by small datasets. However, Brown clustering has high computational complexity and does not lend itself to parallel computation. This, together with the lack of efficient implementations, limits their applicability in NLP. We present efficient implementations of Brown clustering and the alternative Exchange clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}


@article{Kohonen1982,
  title={Self-organized formation of topologically correct feature maps},
  author={Teuvo Kohonen},
  journal={Biological Cybernetics},
  year={1982},
  volume={43},
  pages={59-69}
}

@book{Kohonen2001,
author = {Kohonen, T. and Schroeder, M. R. and Huang, T. S.},
title = {Self-Organizing Maps},
year = {2001},
isbn = {3540679219},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
edition = {3rd}
}

@article{Han2019,
  title={Combining self-organizing maps and biplot analysis to preselect maize phenotypic components based on UAV high-throughput phenotyping platform},
  author={Han, Liang and Yang, Guijun and Dai, Huayang},
  journal={Plant Methods},
  year={2019},
  volume={15},
  pages={57},
SN={1746-4811}
}


@article{SOM2016casestudy,
author = {Pacella, Massimo and Grieco, Antonio and Blaco, Marzia},
title = {On the Use of Self-Organizing Map for Text Clustering in Engineering Change Process Analysis: A Case Study},
year = {2016},
issue_date = {December  2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-5265},
url = {https://doi.org/10.1155/2016/5139574},
doi = {10.1155/2016/5139574},
abstract = {In modern industry, the development of complex products involves engineering changes that frequently require redesigning or altering the products or their components. In an engineering change process, engineering change requests ECRs are documents forms with parts written in natural language describing a suggested enhancement or a problem with a product or a component. ECRs initiate the change process and promote discussions within an organization to help to determine the impact of a change and the best possible solution. Although ECRs can contain important details, that is, recurring problems or examples of good practice repeated across a number of projects, they are often stored but not consulted, missing important opportunities to learn from previous projects. This paper explores the use of Self-Organizing Map SOM to the problem of unsupervised clustering of ECR texts. A case study is presented in which ECRs collected during the engineering change process of a railways industry are analyzed. The results show that SOM text clustering has a good potential to improve overall knowledge reuse and exploitation.},
journal = {Intell. Neuroscience},
month = {dec},
pages = {7}
}

@INPROCEEDINGS{SOM2003,  
author={Hung, C. and Wermter, S.},  
booktitle={Third IEEE International Conference on Data Mining},   
title={A dynamic adaptive self-organising hybrid model for text clustering},   
year={2003},  
pages={75-82},  
doi={10.1109/ICDM.2003.1250905},
abstract={Clustering by document concepts is a powerful way of retrieving information from a large number of documents. This task in general does not make any assumption on the data distribution. For this task we propose a new competitive self-organising (SOM) model, namely the dynamic adaptive self-organising hybrid model (DASH). The features of DASH are a dynamic structure, hierarchical clustering, nonstationary data learning and parameter self-adjustment. All features are data-oriented: DASH adjusts its behaviour not only by modifying its parameters but also by an adaptive structure. The hierarchical growing architecture is a useful facility for such a competitive neural model which is designed for text clustering. We have presented a new type of self-organising dynamic growing neural network which can deal with the nonuniform data distribution and the nonstationary data sets and represent the inner data structure by a hierarchical view}}


@article{SOM2013,
  title={Visualizing the Topical Structure of the Medical Sciences: A Self-Organizing Map Approach},
  author={Andr{\'e} Skupin and Joseph Biberstine and Katy B{\"o}rner},
  journal={PLoS ONE},
  year={2013},
  volume={8},
abstract={Background We implement a high-resolution visualization of the medical knowledge domain using the self-organizing map (SOM) method, based on a corpus of over two million publications. While self-organizing maps have been used for document visualization for some time, (1) little is known about how to deal with truly large document collections in conjunction with a large number of SOM neurons, (2) post-training geometric and semiotic transformations of the SOM tend to be limited, and (3) no user studies have been conducted with domain experts to validate the utility and readability of the resulting visualizations. Our study makes key contributions to all of these issues.}
}



% Financial Text

@misc{OpenEDGAR2008,
  doi = {10.48550/ARXIV.1806.04973},
  url = {https://arxiv.org/abs/1806.04973},
  author = {Bommarito, Michael J and Katz, Daniel Martin and Detterman, Eric M},
  keywords = {Computation and Language (cs.CL), Databases (cs.DB), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7; F.2.2; H.3.1; H.3.3; I.7},
  title = {OpenEDGAR: Open Source Software for SEC EDGAR Analysis},
  publisher = {arXiv},
  year = {2018},  
  copyright = {arXiv.org perpetual, non-exclusive license},
abstract={OpenEDGAR is an open source Python framework designed to rapidly construct research databases based on the Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system operated by the US Securities and Exchange Commission (SEC). OpenEDGAR is built on the Django application framework, supports distributed compute across one or more servers, and includes functionality to (i) retrieve and parse index and filing data from EDGAR, (ii) build tables for key metadata like form type and filer, (iii) retrieve, parse, and update CIK to ticker and industry mappings, (iv) extract content and metadata from filing documents, and (v) search filing document contents. OpenEDGAR is designed for use in both academic research and industrial applications }
}


@article{Loughran2016,
  title={Textual Analysis in Accounting and Finance: A Survey},
  author={Tim Loughran and Bill Mcdonald},
  journal={Behavioral \& Experimental Finance eJournal},
  year={2016},
abstract={Relative to quantitative methods traditionally used in accounting and finance, textual analysis is substantially less precise. Thus, understanding the art is of equal importance to understanding the science. In this survey, we describe the nuances of the method and, as users of textual analysis, some of the tripwires in implementation. We also review the contemporary textual analysis literature and highlight areas of future research.}
}


@ARTICLE{lazyprices2020,
title = {Lazy Prices},
author = {Cohen, Lauren and Malloy, Christopher and Nguyen, Quoc},
year = {2020},
journal = {Journal of Finance},
volume = {75},
number = {3},
pages = {1371-1415},
abstract = {Using the complete history of regular quarterly and annual filings by U.S. corporations, we show that changes to the language and construction of financial reports have strong implications for firms’ future returns and operations. A portfolio that shorts “changers” and buys “nonchangers” earns up to 188 basis points per month in alpha (over 22\% per year) in the future. Moreover, changes to 10‐Ks predict future earnings, profitability, future news announcements, and even future firm‐level bankruptcies. Unlike typical underreaction patterns, we find no announcement effect, suggesting that investors are inattentive to these simple changes across the universe of public firms.},
url = {https://EconPapers.repec.org/RePEc:bla:jfinan:v:75:y:2020:i:3:p:1371-1415}
}


@article{Tueregun2019,
author = {Tuereguen, Nida},
year = {2019},
month = {01},
title = {Text Mining in Financial Information},
Journal = {Current analysis on economics \& finance},
volume = {1},
pages = {18-26},
abstract = {Financial information mainly lies in financial statements, but it should be noted that financial information can be textual as well as
numerals. There are many data sources (footnotes, sustainability reports, executive letters etc.) which are different from the
financial statements that provide useful valuable information for decision makers. As a form of communication, text data allow
researchers to understand managers' behavioral approaches and business behavior. The analysis of these data requires text mining
methods. Text mining is a large data analysis used in the analysis of semi-structural and non-structural data. Within the scope of
this information, it is aimed to raise awareness on the use of text mining in non-structural data analysis in the field of financial
information. In addition, the aim of this study is to provide a review of previous studies and to guide the researchers about its
applications in the field of financial information}
}


@article{Gupta2020,
author = {Gupta, Aaryan and Dengre, Vinya and Kheruwala, Hamza and Shah, Manan},
year = {2020},
month = {11},
pages = {},
title = {Comprehensive review of text-mining applications in finance},
volume = {6},
journal = {Journal of Financial Innovation},
doi = {10.1186/s40854-020-00205-1},
abstract={Text-mining technologies have substantially affected financial industries. As the data in every sector of finance have grown immensely, text mining has emerged as an important field of research in the domain of finance. Therefore, reviewing the recent literature on text-mining applications in finance can be useful for identifying areas for further research. This paper focuses on the text-mining literature related to financial forecasting, banking, and corporate finance. It also analyses the existing literature on text mining in financial applications and provides a summary of some recent studies. Finally, the paper briefly discusses various text-mining methods being applied in the financial domain, the challenges faced in these applications, and the future scope of text mining in finance}
}


@unknown{Ravula2020,
author = {Ravula, Sridhar},
year = {2020},
month = {12},
pages = {},
title = {TEXT ANALYSIS IN FINANCIAL DISCLOSURES},
doi = {10.13140/RG.2.2.26696.75528},
abstract={Financial disclosure analysis and Knowledge extraction is an important financial analysis problem. Prevailing methods depend predominantly on quantitative ratios and techniques, which suffer from limitations like window dressing and past focus. Most of the information in a firm's financial disclosures is in unstructured text and contains valuable information about its health. Humans and machines fail to analyze it satisfactorily due to the enormous volume and unstructured nature, respectively. Researchers have started analyzing text content in disclosures recently. This paper covers the previous work in unstructured data analysis in Finance and Accounting. It also explores the state of art methods in computational linguistics and reviews the current methodologies in Natural Language Processing (NLP). Specifically, it focuses on research related to text source, linguistic attributes, firm attributes, and mathematical models employed in the text analysis approach. This work contributes to disclosure analysis methods by highlighting the limitations of the current focus on sentiment metrics and highlighting broader future research areas}
}

@article{Zhang2018,
author = {Zhang, Shuyu and Aerts, Walter and Pan, Huifeng},
year = {2018},
month = {08},
pages = {},
title = {Causal language intensity in performance commentary and financial analyst behaviour},
volume = {46},
journal = {Journal of Business Finance \& Accounting},
doi = {10.1111/jbfa.12351},
abstract={We use automated techniques to measure causal reasoning on earnings‐related financial outcomes of a large sample of MD&A sections of US firms and examine the intensity of causal language in that context against extent of analyst following and against properties of analysts’ earnings forecasts. We find a positive and significant association between a firm's causal reasoning intensity and analyst following and analyst earnings forecast accuracy respectively. Correspondingly, analysts’ earnings forecast dispersion is negatively and significantly associated with causal reasoning intensity. These results suggest that causal reasoning intensity provides incremental information about the relation between financial performance outcomes and its causes, thereby reducing financial analysts’ information processing and interpreting costs and lowering overall analyst information uncertainty. Additionally, we find that decreases in analyst following are followed by more causal reasoning on performance disclosure. We also find that firms with a considerable increase of causal disclosure especially attract new analysts who already cover many firms. Overall, our evidence of the relation between causal reasoning intensity and properties of analyst behaviour is consistent with the proposition that causal reasoning is a generic narrative disclosure quality characteristic, able to provide incremental information to analysts and guide analysts' behaviour. This article is protected by copyright. All rights reserved}
}


@article{clarkson1998,
author = {Clarkson, Peter and Kao, Jennifer and Gordon, Richardson},
year = {1998},
month = {06},
pages = {111 - 134},
title = {Evidence that Management Discussion and Analysis (MD\&A) Is Part of a Firm's Overall Disclosure Package},
volume = {16},
journal = {Contemporary Accounting Research},
doi = {10.1111/j.1911-3846.1999.tb00576.x},
abstract={The objective of this study is to investigate the role, if any, that management discussion and analysis (MD\&A) plays in a firm's disclosure package. First, we present evidence regarding the usefulness of MD\&A. Our evidence is uniformly supportive of the view that MD\&A is a source of new and useful information and indicates that MD\&A is used for financial analysis purposes by at least one significant user group, sell-side analysts, who are members of the Toronto Society of Financial Analysts. We then provide evidence on disclosure quality. The results reveal that, overall, MD\&A disclosure quality varies with disclosure stimuli similar to those found to influence disclosure choice in other disclosure channels. However, a more refined analysis of the MD\&A subcomponents reveals that different factors influence disclosure quality for those subcomponents. Taken together, our results are consistent with the notion that MD&A is a part of a firm's overall disclosure package.}
}


@misc{finBERT2020,
  doi = {10.48550/ARXIV.2006.08097},  
  url = {https://arxiv.org/abs/2006.08097},
  author = {Yang, Yi and UY, Mark Christopher Siy and Huang, Allen},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FinBERT: A Pretrained Language Model for Financial Communications},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
abstract={Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks}
}



@article{Ahmed2016,
author = {Ahmed, Yousry and Elshandidy, Tamer},
year = {2016},
month = {05},
pages = {},
title = {The effect of bidder conservatism on M\&A decisions: Text-based evidence from US 10-K filings},
volume = {46},
journal = {International Review of Financial Analysis},
doi = {10.1016/j.irfa.2016.05.006},
abstract={This paper examines whether and how bidders’ conservative tone in 10-K filings influences the subsequent mergers and acquisitions investment decisions of these US firms from 1996 to 2013. Based on 39,260 firm-year observations, we find, consistent with behavioural consistency theory, that conservative bidders are less likely to engage in M\&A deals. ... Overall, these findings highlight the implications of the textual sentiment of corporate disclosure for the forecasting of corporate investment and financing decisions. Our results have practical implications, since they shed light on the value relevance of the information content of major Securities Exchange Commission (SEC)-mandated 10-K filings.}
}



@article{Feldman2010,
author = {Feldman, Ronen and Govindaraj, Suresh and Livnat, Joshua and Segal, Benjamin},
year = {2010},
month = {12},
pages = {915-953},
title = {Management's tone change, post earnings announcement drift and accruals. Review of Accounting Studies, 15(4), 915-953},
volume = {15},
journal = {Review of Accounting Studies},
doi = {10.1007/s11142-009-9111-x},
abstract={This study explores whether the management discussion and analysis (MD\&A) section of Forms 10-Q and 10-K has incremental information content beyond financial measures such as earnings surprises and accruals. It uses a classification scheme of words into positive and negative categories to measure the tone change in the MD\&A section relative to prior periodic SEC filings. Our results indicate that short window market reactions around the SEC filing are significantly associated with the tone change of the MD\&A section, even after controlling for accruals and earnings surprises. }
}


@phdthesis{Bochkay2014,
  title={Enhancing empirical accounting models with textual information},
  author={Khrystyna Bochkay},
  year={2014},
  school={Newark Rutgers, The State University of New Jersey},
abstract={We use regularized regression methods to examine whether textual disclosures in the Management Discussion and Analysis (MD\&A) section of the 10-K report are helpful in predicting future earnings above and beyond traditional financial factors.}

}



@article{AmelZadeh2016,
  title={The Information Content of 10-K Narratives: Comparing MD\&A and Footnotes Disclosures},
  author={Amir Amel-Zadeh and Jonathan Faasse},
  journal={Corporate Governance \& Accounting eJournal},
  year={2016},
abstract={This paper examines the textual characteristics of firms’ 10-K filings over a 20 year time period. We find that investors’ reaction to textual characteristics of the MD\&A in 10-Ks is much stronger and more timely than their reaction to textual characteristics of the notes to the financial statements. Characteristics of the MD\&A and footnotes are also predictive of future returns, volatility, and firm profitability. Particularly, changes in the text of the MD\&A and footnotes and differences}
}


@article{TaoDeokar2018,
author = {Jie Tao and Amit V. Deokar and Ashutosh Deshmukh},
title = {Analysing forward-looking statements in initial public offering prospectuses: a text analytics approach},
journal = {Journal of Business Analytics},
volume = {1},
number = {1},
pages = {54-70},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/2573234X.2018.1507604},
URL = { https://doi.org/10.1080/2573234X.2018.1507604},
eprint = { https://doi.org/10.1080/2573234X.2018.1507604 },
    abstract = { ABSTRACTForward-looking statements (FLSs) have informational value in applications such as predicting stock prices. Management Discussion \& Analysis (MD\&A) sections in initial public offering (IPO) prospectuses contain FLSs that provide prospective information about the company’s future growth and performance. This study focuses on evaluating the relationship between features extracted from FLSs and IPO valuation. To that end, we propose an analytical pipeline for identifying FLSs using machine learning techniques. The FLS classifier is built on the best performing deep learning architecture that outperforms extant methods reported in related studies. In order to demonstrate the value of identified FLSs, we conduct predictive analysis of pre-IPO price revisions and post-IPO first-day returns. We engineer a variety of linguistics features from FLSs including topics, sentiments, readability, semantic similarity, and general text features. The study finds that FLS features are more predictive for pre-IPO as compared to post-IPO valuation prediction. The analytical pipeline contributes to the text classification knowledge base while the findings from the predictive analysis shed light on understanding the underpricing phenomenon occurring in the IPO process. }
}


@article{YangDollarMo18,
author = {Yang, Fang and Dolar, Burak and Mo, Lun},
year = {2018},
month = {03},
pages = {},
title = {Textual Analysis of Corporate Annual Disclosures: A Comparison Between Bankrupt and Non-Bankrupt Companies},
volume = {15},
journal = {Journal of Emerging Technologies in Accounting},
doi = {10.2308/jeta-52085},
abstract={In this study we focus on extracting qualitative information from the management discussion and analysis section of an annual report and compare whether there are textually evident differences in textual expressions used between bankrupt and non-bankrupt companies. We extract high-frequency words, related concept links, and topics from MD\&As and find that some high-frequency words appear to suggest differences between bankrupt and non-bankrupt companies regarding their financial position and ongoing status. However, the usefulness of concept links is mixed. Some concept links for high-frequency words do not seem to center around a theme or a key word, yet others provide some contextual information supporting our conjectures about the ongoing business status of non-bankrupt companies. Finally, we perform topic extraction based on a latent semantic analysis algorithm in order to investigate whether issues and themes discussed differ between non-bankrupt and bankrupt companies. We find that most of the top topics extracted merely recapture the characteristics of industries in which companies operate and do not provide information in differentiating between bankrupt and non-bankrupt companies. The reasons are discussed in the paper.}
}


@article{BourveauLouWang18,
athor = {Thomas Bourveau and Yun Lou and Rencheng Wang},
year={2018},
title={Shareholder Litigation and Corporate Disclosure: Evidence from Derivative Lawsuits},
journal={Journal of Accounting Research},
publisher={Wiley Blackwell}, 
volume ={56(3)}, 
pages = {797-842},
month = {June},
abstract={Using the staggered adoption of universal demand (UD) laws in the United States, we study the effect of shareholder litigation risk on corporate disclosure. We find that disclosure significantly increases after UD laws make it more difficult to file derivative lawsuits. Specifically, firms issue more earnings forecasts and voluntary 8‐K filings, and increase the length of management discussion and analysis (MD\&A) in their 10‐K filings. }
}





% explainable AI

@Article{explainableAI2021,
AUTHOR = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
TITLE = {Explainable {AI}: A Review of Machine Learning Interpretability Methods},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/1099-4300/23/1/18},
PubMedID = {33375658},
ISSN = {1099-4300},
ABSTRACT = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into black box; approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
DOI = {10.3390/e23010018}
}



% SOM rule of the thumb
@article{SOM2019,
title = {Self-Organizing Maps and Their Applications to Data Analysis},
author = {Ponmalai, Ravi and Kamath, Chandrika},
abstractNote = {Self-Organizing Maps(SOMs) are a form of unsupervised neural network that are used for visualization and exploratory data analysis of high dimensional datasets. Our goal was to understand how we can use a SOM to gain insights about datasets. We do this by first understanding the initialization, training, error metrics, and convergence properties of the SOM. Next we discuss the ways to interpret and visualize a Self-Organizing Map. Finally we used real datasets to understand what the Self-Organizing Map can tell us about labeled and unlabeled data. Based on experiments with our datasets we found that the Self-Organizing Map can tell us about the spacing and position of high dimensional clusters, help us find non-linear patterns, and give us insight into the shape of our data.},
doi = {10.2172/1566795},
url = {https://www.osti.gov/biblio/1566795}, journal = {},
place = {United States},
year = {2019},
month = {9}
}


@inproceedings{SOM2014,
  title={Anomaly Detection Using Self-Organizing Maps-Based K-Nearest Neighbor Algorithm},
  author={Jing Tian and Michael H. Azarian and Michael G. Pecht},
  year={2014},
journal = {PHM Society European Conference, 2(1).},
doi = {https://doi.org/10.36001/phme.2014.v2i1.1554}
}


@inproceedings{SOM2017,
  title={Evaluating Self-Organizing Map Quality Measures as Convergence Criteria},
  author={Gregory Breard},
  year={2017},
  journal = {Open Access Master's Theses. Paper 1033},
  doi = {https://doi.org/10.23860/thesis-breard-gregory-2017}

}


@inproceedings{elbow2020,
  title={K-Means Clustering Optimization Using the Elbow Method and Early Centroid Determination Based on Mean and Median Formula},
  author={Edy Umargono and Jatmiko Endro Suseno and S.K Vincensius Gunawan},
  year={2020},
  booktitle={Proceedings of the 2nd International Seminar on Science and Technology (ISSTEC 2019)},
  pages={121-129},
  issn={2352-5398},
  isbn={978-94-6239-168-0},
  url={https://doi.org/10.2991/assehr.k.201010.019},
  doi={https://doi.org/10.2991/assehr.k.201010.019},
  publisher={Atlantis Press}
}


% complexity

@article{clustering1999,
author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
title = {Data Clustering: A Review},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/331499.331504},
doi = {10.1145/331499.331504},
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview
of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify
 cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
journal = {ACM Comput. Surv.},
month = {sep},
pages = {264-323},
numpages = {60},
keywords = {incremental clustering, exploratory data analysis, clustering applications, cluster analysis, unsupervised learning, similarity indices}
}


@inproceedings{complexity2017,
  author={Melka, Josu{\'e} and Mariage, Jean-Jacques},
  title={Efficient Implementation of Self-Organizing Map for Sparse Input Data},
  booktitle={Proceedings of the 9th International Joint Conference on Computational Intelligence: IJCCI},
  volume={1},
  month={November},
  year={2017},
  address={Funchal, Madeira, Portugal},
  pages={54-63},
  publisher={SciTePress},
  organization={INSTICC},
  doi={10.5220/0006499500540063},
  isbn={978-989-758-274-5},
  url={http://www.ai.univ-paris8.fr/~jmelka/IJCCI_2017_20.pdf}
}
  title={Efficient Implementation of Self-Organizing Map for Sparse Input Data},
  author={Josu{\'e} Melka and Jean-Jacques Mariage},
  booktitle={IJCCI},
  year={2017}
}


@article{similarity2020,
Author = {Ontanon, Santiago},
year = {2020},
month = {October},
title = {An overview of distance and similarity functions for structured data},
journal ={Artificial Intelligence Review},
pages = {5309 - 5351},
volumen = {53},
Issue = {7},
Abstract ={The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work.},
Url ={https://doi.org/10.1007/s10462-020-09821-w},
DOI = {10.1007/s10462-020-09821-w}
}


@inbook{Aggarwal2012,
  author = {Aggarwal, Charu C. and Zhai, ChengXiang},
  booktitle = {Mining Text Data},
  doi = {10.1007/978-1-4614-3223-4_4},
  editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
  isbn = {978-1-4614-3223-4},
  pages = {77 - 128},
  publisher = {Springer US},
  timestamp = {2017-06-18T18:49:34.000+0200},
  title = {A Survey of Text Clustering Algorithms},
  url = {http://dx.doi.org/10.1007/978-1-4614-3223-4_4},
  year = {2012},
abstract = {Clustering is a widely studied data mining problem in the text domains. The problem finds numerous applications in customer segmentation, classification, collaborative filtering, visualization, document organization, and indexing. In this chapter, we will provide a detailed survey of the problem of text clustering. We will study the key challenges of the clustering problem, as it applies to the text domain. We will discuss th 
key methods used for text clustering, and their relative advantages. We will also discuss a number of recent advances in the area in the context
of social network and linked data.}

}


@misc{surveyTextMining2017,
  doi = {10.48550/ARXIV.1707.02919},
  url = {https://arxiv.org/abs/1707.02919},
  author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saied and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
  title = {A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques},  
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
abstract={The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.}
}

@inproceedings{surveyDocClustering2015,
author = {Naik, Maitri and Prajapati, Harshadkumar and Dabhi, Vipul},
year = {2015},
month = {03},
pages = {},
title = {A survey on semantic document clustering},
doi = {10.1109/ICECCT.2015.7226036}
}



%Related work
@mastersthesis{CompanyKG2021,
  title={Exploring Construction of a Company Domain-Specific Knowledge Graph from Financial Texts Using Hybrid Information Extraction},
  author={Jen, Chun-Heng},
  year={2021},
  school={KTH, School of Electrical Engineering and Computer Science (EECS)},
abstract = {Mapping a given company’s relationships with other companies in terms of competitors, subsidiaries, suppliers, and customers are key to understanding a company's major risk factors and opportunities. Conventionally, obtaining and staying up to date with this key knowledge was achieved by reading financial news and reports by highly skilled manual labor like a financial analyst. However, with the development of Natural Language Processing (NLP) and graph databases, it is now possible to systematically extract and store structured information from unstructured data sources. The current go-to method to effectively extract information uses supervised machine learning models, which require a large amount of labeled training data. The data labeling process is usually time-consuming and hard to get in a domain-specific area. 
This project explores an approach to construct a company domain-specific Knowledge Graph that contains company-related entities and relationships from 10-K filings by combining a pre-trained general NLP with rule-based patterns in Named Entity Recognition (NER) and Relation Extraction (RE). This approach eliminates the time-consuming data-labeling task in the statistical approach, and by evaluating ten 10-k filings, the model has the overall Recall of 53.6\%, Precision of 75.7\%, and the F1-score of 62.8\%. The result shows it is possible to extract company information using the hybrid methods, which does not require a large amount of labeled training data. However, the project requires the time-consuming process of finding lexical patterns from sentences to extract company-related entities and relationships. }
}


@inproceedings{MappingKG2018,
  title={Mapping Deep NLP to Knowledge Graphs : An Enhanced Approach to Analyzing Corporate Filings with Regulators},
  author={Damir Cavar and Matthew Josefy},
  year={2018},
  booktitle = {Proceedings of the Language Resources and Evaluation Conference (LREC 2018), The 1st Financial Narrative Processing Workshop (FNP 2018)},
    address = {Miyazaki, Japan},
    abstract = {Filings submitted by companies to the Securities and Exchange Commission provide a tremendous corpus for application of advanced natural language processing techniques. While business scholars actively utilize these texts, interdisciplinary efforts hold substantial promise to advance knowledge and techniques. In this study, we utilize deep natural language processing techniques to extract meaningful knowledge from SEC filings. We construct a comprehensive pipeline that extracts the original filings, processes them in order to recognize component segments for distinct analysis, feeds each text through multiple NLP processors to obtain optimal
recognition of the linguistic properties, and ultimately seeks to construct a comprehensive knowledge graph of how companies, their executives and their directors are linked to one another, or how various risks are identified, weighted, and handled over long periods of time. We thus link advanced NLP techniques and knowledge graphing approaches, contributing greater domain specific knowledge to
advance linguistic approaches and potentially discovering underlying networks that would be difficult to detect with other approaches}
}



















